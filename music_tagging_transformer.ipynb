{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music tagging Transformer\n",
    "This file traines a transformer from the melspectogram features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import save_utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    GlobalAvgPool1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from transformer import Encoder\n",
    "\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import clip_ops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define crossentropy and accuracy metric for the training routine\n",
    "For the metric we use a binary accuracy, for the loss a binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    threshold = math_ops.cast(threshold, y_pred.dtype)\n",
    "    y_pred = math_ops.cast(y_pred > threshold, y_pred.dtype)\n",
    "    y_true = math_ops.cast(y_true > threshold, y_true.dtype)\n",
    "\n",
    "    return K.mean(math_ops.equal(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "    epsilon_ = K._constant_to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    output = clip_ops.clip_by_value(y_pred, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "    # Compute cross entropy from probabilities.\n",
    "    bce = 4 * y_true * math_ops.log(output + K.epsilon())\n",
    "    bce += (1 - y_true) * math_ops.log(1 - output + K.epsilon())\n",
    "    return K.sum(-bce, axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "Define the transformer model structure with the Encoder from the transformer util file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(model_config, n_classes):\n",
    "    num_layers = model_config['n_layers']\n",
    "    d_model = model_config['d_model']\n",
    "    num_heads = model_config['n_heads']\n",
    "    dff = model_config['dff']\n",
    "    maximum_position_encoding = model_config['max_pos_encoding']\n",
    "    init_lr = model_config['init_learning_rate']\n",
    "    dropout_rate = model_config['dropout_rate']\n",
    "    activations = model_config['activations']\n",
    "\n",
    "    input_layer = Input((None, d_model))\n",
    "\n",
    "    encoder = Encoder(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        maximum_position_encoding=maximum_position_encoding,\n",
    "        rate=model_config['encoder_rate']\n",
    "    )\n",
    "\n",
    "    x = encoder(input_layer)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GlobalAvgPool1D()(x)\n",
    "    x = Dense(4 * n_classes, activation=activations[0])(x)\n",
    "\n",
    "    out = Dense(n_classes, activation=activations[1])(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=out)\n",
    "    model.compile(optimizer=Adam(init_lr), loss=custom_binary_crossentropy, metrics=[custom_binary_accuracy])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "## Load the configuration for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the config\n",
    "with open('music_tag_transformer/transformer_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract the values\n",
    "transformer_name = config['transformer_name']\n",
    "transformer_pretrained_name = config['pretrained_transformer']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "data_dir = Path(config['data_dir'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data\n",
    "Load the numpy arrays and the label-class mapping. Split the data accordingly into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_data = save_utils.load_sliced_numpy_array('melspec_features', data_dir=data_dir)\n",
    "labels = np.load(data_dir/'labels.npy')\n",
    "\n",
    "with open(data_dir/'class_label_index_mapping.json', 'r') as f:\n",
    "    labels_to_id = json.load(f)\n",
    "\n",
    "mel_train, mel_test_val, lab_train, lab_test_val = train_test_split(melspec_data, labels, train_size=config['train_set_size'], random_state=config['random_state'])\n",
    "mel_val, mel_test, lab_val, lab_test             = train_test_split(mel_test_val, lab_test_val, test_size=(config['val_set_size']/(1-config['train_set_size'])), shuffle=False)\n",
    "\n",
    "# Check the shapes of the splitted sets\n",
    "assert mel_train.shape[0] == lab_train.shape[0] and mel_test.shape[0] == lab_test.shape[0] and mel_val.shape[0] == lab_val.shape[0]\n",
    "assert mel_train.shape[1] == mel_test.shape[1] == mel_val.shape[1] and lab_train.shape[1] == lab_test.shape[1] == lab_val.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model callbacks\n",
    "For the initializing parameters, refer to the config file.  \n",
    "For callbacks, we use a frequently backup of the model as checkpoints, and early stopping mechanism to prevent overfitting on the training data and a learningrate reducer, that smallers the update steps when the validation metric does not improve any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config['training']\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    transformer_name,\n",
    "    monitor=train_config['monitor'],\n",
    "    verbose=1,\n",
    "    save_best_only=train_config['save_best_weights'],\n",
    "    mode=train_config['monitor_mode'],\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    mode = 'min',\n",
    "    restore_best_weights = True,\n",
    "    start_from_epoch = 5\n",
    ")\n",
    "\n",
    "# Reduce learning rate when val_loss stopps improving\n",
    "lr_reduce_config = train_config['lr_reducing']\n",
    "lr_reducing_on_platteau = ReduceLROnPlateau(\n",
    "    monitor=lr_reduce_config['monitor'], patience=lr_reduce_config['patience'], min_lr=lr_reduce_config['min_lr'], mode=lr_reduce_config['mode']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the config\n",
    "with open('preprocess_config.yaml', 'r') as f:\n",
    "    pp_config = yaml.safe_load(f)\n",
    "\n",
    "n_mels = pp_config['melspectogram']['n_mels']\n",
    "mel_train = mel_train.reshape(mel_train.shape[0], -1, )\n",
    "mel_val   = mel_val.reshape(mel_val.shape[0], -1, )\n",
    "\n",
    "model = transformer_model(config['model_structure'], n_classes=len(labels_to_id))\n",
    "\n",
    "history = model.fit(\n",
    "        x=mel_train,\n",
    "        y=lab_train,\n",
    "        validation_data=(mel_val, lab_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, lr_reducing_on_platteau, early_stopping],\n",
    "        use_multiprocessing=True,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "model.save('models/complete'+transformer_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_industry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d291c4e3f50793f536898541957e0ac77bd589a86ab1345b9dadb714b5a442d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
