batch_size: 128
epochs: 40
transformer_name: my_transformer
random_state: 420
train_set_size: 0.6
val_set_size: 0.2
data_dir: data
model_structure:
  n_layers: 4
  d_model: 128
  n_heads: 8
  dff: 256
  max_pos_encoding: 2048
  init_learning_rate: 0.0001
  dropout_rate: 0.2
  encoder_rate: 0.3
  activations: [selu, sigmoid]
training:
  monitor: val_loss
  monitor_mode: min
  save_best_weights: True
  lr_reducing:
    monitor: val_loss
    patience: 5
    min_lr: 1.e-7
    mode: min