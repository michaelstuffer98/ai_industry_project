{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "This file traines a recurrent neural network on the melspectogram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 23:27:20.838999: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from lib_util import utils, plot\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping \n",
    "\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    GlobalAvgPool1D,\n",
    "    Dense,\n",
    "    Bidirectional,\n",
    "    GRU,\n",
    "    Dropout,\n",
    ")\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import clip_ops\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define crossentropy and accuracy metric for the training routine\n",
    "\n",
    "For the metric we use a binary accuracy, for the loss a binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculating the accuracy for the model where the treshold is 0.5 and the result will be between 0 and 1 \n",
    "    \"\"\"\n",
    "    threshold = math_ops.cast(threshold, y_pred.dtype)\n",
    "    y_pred = math_ops.cast(y_pred > threshold, y_pred.dtype)\n",
    "    y_true = math_ops.cast(y_true > threshold, y_true.dtype)\n",
    "\n",
    "    return K.mean(math_ops.equal(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Calculating the cross entropy from probabilities where the result will be between 0 and 1\n",
    "    \"\"\"\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "    epsilon_ = K._constant_to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    output = clip_ops.clip_by_value(y_pred, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "    # Compute cross entropy from probabilities.\n",
    "    bce = 4 * y_true * math_ops.log(output + K.epsilon())\n",
    "    bce += (1 - y_true) * math_ops.log(1 - output + K.epsilon())\n",
    "    return K.sum(-bce, axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "define the RNN model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(model_config, n_classes):\n",
    "    \"\"\"RNN model where the model will be trained on the training data\n",
    "    \"\"\"\n",
    "    d_model = model_config['d_model']\n",
    "    n_layers = model_config['n_layers']\n",
    "    init_lr = model_config['init_learning_rate']\n",
    "    dropout_rate = model_config['dropout_rate']\n",
    "    activations = model_config['activations']\n",
    "    inp = Input((None, d_model))\n",
    "    #Bidirectional means having a neural network in both directions backwards\n",
    "    x = Bidirectional(GRU(d_model, return_sequences=True))(inp)\n",
    "    # Making different bidirectional layers \n",
    "    if n_classes > 1:\n",
    "        for i in range(n_layers - 3):\n",
    "            x = Bidirectional(GRU(d_model, return_sequences=True))(x)\n",
    "\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GlobalAvgPool1D()(x)\n",
    "    x = Dense(4 * n_classes, activation=activations[0])(x)\n",
    "    out = Dense(n_classes, activation=activations[1])(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=Adam(init_lr), loss=custom_binary_crossentropy, metrics=[custom_binary_accuracy]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "Load the configuration for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config('rnn')\n",
    "\n",
    "# Extract the values\n",
    "model_name = config['model_name']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "data_dir = Path(config['data_dir'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "\n",
    "Load the numpy arrays and the label-class mapping. Split the data accordingly into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 files:\n",
      "    melspec_features_001.npy\n",
      "    melspec_features_002.npy\n",
      "    melspec_features_003.npy\n",
      "    melspec_features_007.npy\n",
      "    melspec_features_012.npy\n",
      "    melspec_features_006.npy\n",
      "    melspec_features_010.npy\n",
      "    melspec_features_004.npy\n",
      "    melspec_features_005.npy\n",
      "    melspec_features_011.npy\n",
      "    melspec_features_008.npy\n",
      "    melspec_features_009.npy\n"
     ]
    }
   ],
   "source": [
    "melspec_data = utils.load_sliced_numpy_array('melspec_features', data_dir=data_dir)\n",
    "labels = np.load(data_dir/'labels.npy')\n",
    "\n",
    "labels_to_id = utils.get_class_mapping()\n",
    "\n",
    "mel_train, mel_test_val, lab_train, lab_test_val = train_test_split(melspec_data, labels, train_size=config['train_set_size'], random_state=config['random_state'])\n",
    "mel_val, mel_test, lab_val, lab_test             = train_test_split(mel_test_val, lab_test_val, test_size=(config['val_set_size']/(1-config['train_set_size'])), shuffle=False)\n",
    "\n",
    "# Check the shapes of the splitted sets\n",
    "assert mel_train.shape[0] == lab_train.shape[0] and mel_test.shape[0] == lab_test.shape[0] and mel_val.shape[0] == lab_val.shape[0]\n",
    "assert mel_train.shape[1] == mel_test.shape[1] == mel_val.shape[1] and lab_train.shape[1] == lab_test.shape[1] == lab_val.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model callbacks\n",
    "\n",
    "For initializing parameters, refer to the config file. For callbacks, we use a frequently backup of the model as checkpoints, and early stopping mechanism to prevent overfitting on the train data and a learning rate reducer. The learning rate reducer smallers the update step when the validation metric does not improve anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config['training']\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_name,\n",
    "    monitor=train_config['monitor'],\n",
    "    verbose=1,\n",
    "    save_best_only=train_config['save_best_weights'],\n",
    "    mode=train_config['monitor_mode'],\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    mode = 'min',\n",
    "    restore_best_weights = True,\n",
    "    start_from_epoch = 5\n",
    ")\n",
    "\n",
    "# Reduce learning rate when val_loss stopps improving\n",
    "lr_reduce_config = train_config['lr_reducing']\n",
    "lr_reducing_on_platteau = ReduceLROnPlateau(\n",
    "    monitor=lr_reduce_config['monitor'], patience=lr_reduce_config['patience'], min_lr=lr_reduce_config['min_lr'], mode=lr_reduce_config['mode']\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the config\n",
    "pp_config = utils.get_config('preprocess')\n",
    "\n",
    "n_mels = pp_config['melspectogram']['n_mels']\n",
    "mel_train = mel_train.reshape(mel_train.shape[0], -1, n_mels)\n",
    "mel_val   = mel_val.reshape(mel_val.shape[0], -1, n_mels)\n",
    "\n",
    "model = rnn_model(config['model_structure'], n_classes=len(labels_to_id))\n",
    "\n",
    "history = model.fit(\n",
    "        x=mel_train,\n",
    "        y=lab_train,\n",
    "        validation_data=(mel_val, lab_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, lr_reducing_on_platteau, early_stopping],\n",
    "        use_multiprocessing=True,\n",
    "        verbose=2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dump = {\n",
    "    'model': model,\n",
    "    'history': history,\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "utils.save_training(to_dump, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = utils.load_history(model_name)\n",
    "\n",
    "#Plots for the accuracies and losses of the train and validation data per epoch\n",
    "plot.plot_hist(history, ('accuracy', 'val_accuracy'), legends=('train', 'validation'), title='Accuracy', y_label='accuracy ->', x_label='epochs ->', save_to=f'Plots/short_chunk_cnn_{epochs}_acuracy')\n",
    "plot.plot_hist(history, ('loss', 'val_loss'), legends=('train', 'validation'), title='Loss', y_label='loss ->', x_label='epochs ->', save_to=f'Plots/short_chunk_cnn_{epochs}_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lambda model, data: np.argmax(model.predict(data), axis=-1)\n",
    "\n",
    "#Loading in the model\n",
    "model = utils.load_model(model_name)\n",
    "\n",
    "# Training prediction\n",
    "y_pred_train = predict(model, mel_train)\n",
    "y_true_train = np.argmax(lab_train, axis= -1)\n",
    "print(f\"ACCURACY FOR TRAIN SET {accuracy_score(y_true_train, y_pred_train)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='micro')*100:.4f} %\")\n",
    "\n",
    "\n",
    "# Validation prediction\n",
    "y_pred_val = predict(model, mel_val)\n",
    "y_true_val = np.argmax(lab_val, axis= -1)\n",
    "print(f\"ACCURACY FOR VAL SET {accuracy_score(y_true_val, y_pred_val)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='micro')*100:.4f} %\")\n",
    "\n",
    "# Test prediction\n",
    "y_pred_test = predict(model, mel_test)\n",
    "y_true_test = np.argmax(lab_test, axis= -1)\n",
    "print(f\"ACCURACY FOR TEST SET {accuracy_score(y_true_test, y_pred_test)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='micro')*100:.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = utils.get_class_names()\n",
    "\n",
    "#Confusion matrix of the predicted labels versus the true labels\n",
    "conf_mat = confusion_matrix(y_true_test, y_pred_test, normalize= 'true')\n",
    "conf_mat = np.round(conf_mat, 2)\n",
    "\n",
    "conf_mat_df = pd.DataFrame(conf_mat, columns=class_names, index=class_names)\n",
    "\n",
    "plot_conf_mat(conf_mat_df, save_to=f\"Plots/{model_name}{epochs}_test_conf_mat.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-industry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8db3ad7877ac29be891f762e2f8f2bd0b14f50820e5ea7ad73e7636a7ffe5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
