{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "This file traines a recurrent neural network on the melspectogram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import save_utils\n",
    "from plot import plot_conf_mat, plot_hist\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping \n",
    "\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    GlobalAvgPool1D,\n",
    "    Dense,\n",
    "    Bidirectional,\n",
    "    GRU,\n",
    "    Dropout,\n",
    ")\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import clip_ops\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define crossentropy and accuracy metric for the training routine\n",
    "\n",
    "For the metric we use a binary accuracy, for the loss a binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculating the accuracy for the model where the treshold is 0.5 and the result will be between 0 and 1 \n",
    "    \"\"\"\n",
    "    threshold = math_ops.cast(threshold, y_pred.dtype)\n",
    "    y_pred = math_ops.cast(y_pred > threshold, y_pred.dtype)\n",
    "    y_true = math_ops.cast(y_true > threshold, y_true.dtype)\n",
    "\n",
    "    return K.mean(math_ops.equal(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "def custom_binary_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Calculating the cross entropy from probabilities where the result will be between 0 and 1\n",
    "    \"\"\"\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "    epsilon_ = K._constant_to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    output = clip_ops.clip_by_value(y_pred, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "    # Compute cross entropy from probabilities.\n",
    "    bce = 4 * y_true * math_ops.log(output + K.epsilon())\n",
    "    bce += (1 - y_true) * math_ops.log(1 - output + K.epsilon())\n",
    "    return K.sum(-bce, axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "define the RNN model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(model_config, n_classes):\n",
    "    \"\"\"RNN model where the model will be trained on the training data\n",
    "    \"\"\"\n",
    "    d_model = model_config['d_model']\n",
    "    n_layers = model_config['n_layers']\n",
    "    init_lr = model_config['init_learning_rate']\n",
    "    dropout_rate = model_config['dropout_rate']\n",
    "    activations = model_config['activations']\n",
    "    inp = Input((None, d_model))\n",
    "    #Bidirectional means having a neural network in both directions backwards\n",
    "    x = Bidirectional(GRU(d_model, return_sequences=True))(inp)\n",
    "    # Making different bidirectional layers \n",
    "    if n_classes > 1:\n",
    "        for i in range(n_layers - 3):\n",
    "            x = Bidirectional(GRU(d_model, return_sequences=True))(x)\n",
    "\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GlobalAvgPool1D()(x)\n",
    "    x = Dense(4 * n_classes, activation=activations[0])(x)\n",
    "    out = Dense(n_classes, activation=activations[1])(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=Adam(init_lr), loss=custom_binary_crossentropy, metrics=[custom_binary_accuracy]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "Load the configuration for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the config\n",
    "with open('rnn_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract the values\n",
    "model_name = config['model_name']\n",
    "rnn_name = config['rnn_name']\n",
    "rnn_pretrained_name = config['pretrained_rnn']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']\n",
    "data_dir = Path(config['data_dir'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "\n",
    "Load the numpy arrays and the label-class mapping. Split the data accordingly into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 files:\n",
      "    melspec_features_001.npy\n",
      "    melspec_features_002.npy\n",
      "    melspec_features_003.npy\n",
      "    melspec_features_007.npy\n",
      "    melspec_features_012.npy\n",
      "    melspec_features_006.npy\n",
      "    melspec_features_010.npy\n",
      "    melspec_features_004.npy\n",
      "    melspec_features_005.npy\n",
      "    melspec_features_011.npy\n",
      "    melspec_features_008.npy\n",
      "    melspec_features_009.npy\n"
     ]
    }
   ],
   "source": [
    "melspec_data = save_utils.load_sliced_numpy_array('melspec_features', data_dir=data_dir)\n",
    "labels = np.load(data_dir/'labels.npy')\n",
    "\n",
    "with open(data_dir/'class_label_index_mapping.json', 'r') as f:\n",
    "    labels_to_id = json.load(f)\n",
    "\n",
    "mel_train, mel_test_val, lab_train, lab_test_val = train_test_split(melspec_data, labels, train_size=config['train_set_size'], random_state=config['random_state'])\n",
    "mel_val, mel_test, lab_val, lab_test             = train_test_split(mel_test_val, lab_test_val, test_size=(config['val_set_size']/(1-config['train_set_size'])), shuffle=False)\n",
    "\n",
    "# Check the shapes of the splitted sets\n",
    "assert mel_train.shape[0] == lab_train.shape[0] and mel_test.shape[0] == lab_test.shape[0] and mel_val.shape[0] == lab_val.shape[0]\n",
    "assert mel_train.shape[1] == mel_test.shape[1] == mel_val.shape[1] and lab_train.shape[1] == lab_test.shape[1] == lab_val.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model callbacks\n",
    "\n",
    "For initializing parameters, refer to the config file. For callbacks, we use a frequently backup of the model as checkpoints, and early stopping mechanism to prevent overfitting on the train data and a learning rate reducer. The learning rate reducer smallers the update step when the validation metric does not improve anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config['training']\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    rnn_name,\n",
    "    monitor=train_config['monitor'],\n",
    "    verbose=1,\n",
    "    save_best_only=train_config['save_best_weights'],\n",
    "    mode=train_config['monitor_mode'],\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    mode = 'min',\n",
    "    restore_best_weights = True,\n",
    "    start_from_epoch = 5\n",
    ")\n",
    "\n",
    "# Reduce learning rate when val_loss stopps improving\n",
    "lr_reduce_config = train_config['lr_reducing']\n",
    "lr_reducing_on_platteau = ReduceLROnPlateau(\n",
    "    monitor=lr_reduce_config['monitor'], patience=lr_reduce_config['patience'], min_lr=lr_reduce_config['min_lr'], mode=lr_reduce_config['mode']\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None, 128)]       0         \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, None, 256)        198144    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirecti  (None, None, 256)        296448    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 256)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 20)                5140      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 499,837\n",
      "Trainable params: 499,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.94855, saving model to rnn.h5\n",
      "77/77 - 66s - loss: 5.4414 - custom_binary_accuracy: 0.5782 - val_loss: 4.9485 - val_custom_binary_accuracy: 0.6265 - lr: 0.0010 - 66s/epoch - 859ms/step\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 2: val_loss improved from 4.94855 to 4.88575, saving model to rnn.h5\n",
      "77/77 - 54s - loss: 4.9917 - custom_binary_accuracy: 0.6152 - val_loss: 4.8858 - val_custom_binary_accuracy: 0.6494 - lr: 0.0010 - 54s/epoch - 698ms/step\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 3: val_loss improved from 4.88575 to 4.71981, saving model to rnn.h5\n",
      "77/77 - 53s - loss: 4.6400 - custom_binary_accuracy: 0.6552 - val_loss: 4.7198 - val_custom_binary_accuracy: 0.5823 - lr: 0.0010 - 53s/epoch - 687ms/step\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 4: val_loss improved from 4.71981 to 4.53273, saving model to rnn.h5\n",
      "77/77 - 60s - loss: 4.4140 - custom_binary_accuracy: 0.6705 - val_loss: 4.5327 - val_custom_binary_accuracy: 0.6334 - lr: 0.0010 - 60s/epoch - 780ms/step\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 5: val_loss improved from 4.53273 to 4.47261, saving model to rnn.h5\n",
      "77/77 - 63s - loss: 4.2077 - custom_binary_accuracy: 0.6791 - val_loss: 4.4726 - val_custom_binary_accuracy: 0.6565 - lr: 0.0010 - 63s/epoch - 819ms/step\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 6: val_loss did not improve from 4.47261\n",
      "77/77 - 61s - loss: 3.9451 - custom_binary_accuracy: 0.7097 - val_loss: 4.5931 - val_custom_binary_accuracy: 0.6425 - lr: 0.0010 - 61s/epoch - 795ms/step\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 7: val_loss did not improve from 4.47261\n",
      "77/77 - 62s - loss: 3.7921 - custom_binary_accuracy: 0.7190 - val_loss: 4.7795 - val_custom_binary_accuracy: 0.7071 - lr: 0.0010 - 62s/epoch - 803ms/step\n",
      "Epoch 8/40\n",
      "\n",
      "Epoch 8: val_loss did not improve from 4.47261\n",
      "77/77 - 60s - loss: 3.6248 - custom_binary_accuracy: 0.7335 - val_loss: 5.0453 - val_custom_binary_accuracy: 0.6796 - lr: 0.0010 - 60s/epoch - 785ms/step\n",
      "Epoch 9/40\n",
      "\n",
      "Epoch 9: val_loss did not improve from 4.47261\n",
      "77/77 - 63s - loss: 3.4558 - custom_binary_accuracy: 0.7489 - val_loss: 4.7479 - val_custom_binary_accuracy: 0.7059 - lr: 0.0010 - 63s/epoch - 822ms/step\n",
      "Epoch 10/40\n",
      "\n",
      "Epoch 10: val_loss did not improve from 4.47261\n",
      "77/77 - 58s - loss: 3.1549 - custom_binary_accuracy: 0.7774 - val_loss: 5.0906 - val_custom_binary_accuracy: 0.7076 - lr: 0.0010 - 58s/epoch - 757ms/step\n",
      "Epoch 11/40\n",
      "\n",
      "Epoch 11: val_loss did not improve from 4.47261\n",
      "77/77 - 58s - loss: 3.0276 - custom_binary_accuracy: 0.7946 - val_loss: 5.1412 - val_custom_binary_accuracy: 0.6708 - lr: 0.0010 - 58s/epoch - 753ms/step\n",
      "Epoch 12/40\n",
      "\n",
      "Epoch 12: val_loss did not improve from 4.47261\n",
      "77/77 - 60s - loss: 2.7804 - custom_binary_accuracy: 0.8127 - val_loss: 5.7356 - val_custom_binary_accuracy: 0.6941 - lr: 0.0010 - 60s/epoch - 783ms/step\n",
      "Epoch 13/40\n",
      "\n",
      "Epoch 13: val_loss did not improve from 4.47261\n",
      "77/77 - 61s - loss: 2.5634 - custom_binary_accuracy: 0.8341 - val_loss: 6.0235 - val_custom_binary_accuracy: 0.6971 - lr: 0.0010 - 61s/epoch - 791ms/step\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 14: val_loss did not improve from 4.47261\n",
      "77/77 - 63s - loss: 2.4463 - custom_binary_accuracy: 0.8398 - val_loss: 6.3431 - val_custom_binary_accuracy: 0.7145 - lr: 0.0010 - 63s/epoch - 817ms/step\n",
      "Epoch 15/40\n",
      "\n",
      "Epoch 15: val_loss did not improve from 4.47261\n",
      "77/77 - 62s - loss: 2.5501 - custom_binary_accuracy: 0.8334 - val_loss: 6.0924 - val_custom_binary_accuracy: 0.7044 - lr: 0.0010 - 62s/epoch - 800ms/step\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 16: val_loss did not improve from 4.47261\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "77/77 - 58s - loss: 2.2284 - custom_binary_accuracy: 0.8561 - val_loss: 6.1215 - val_custom_binary_accuracy: 0.7106 - lr: 0.0010 - 58s/epoch - 751ms/step\n",
      "Epoch 16: early stopping\n"
     ]
    }
   ],
   "source": [
    "# read in the config\n",
    "with open('preprocess_config.yaml', 'r') as f:\n",
    "    pp_config = yaml.safe_load(f)\n",
    "\n",
    "n_mels = pp_config['melspectogram']['n_mels']\n",
    "mel_train = mel_train.reshape(mel_train.shape[0], -1, n_mels)\n",
    "mel_val   = mel_val.reshape(mel_val.shape[0], -1, n_mels)\n",
    "\n",
    "model = rnn_model(config['model_structure'], n_classes=len(labels_to_id))\n",
    "\n",
    "history = model.fit(\n",
    "        x=mel_train,\n",
    "        y=lab_train,\n",
    "        validation_data=(mel_val, lab_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, lr_reducing_on_platteau, early_stopping],\n",
    "        use_multiprocessing=True,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "model.save('models/complete'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dump = {\n",
    "    'model': model,\n",
    "    'history': history,\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "save_utils.save_training(to_dump, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = save_utils.load_history(model_name)\n",
    "\n",
    "#Plots for the accuracies and losses of the train and validation data per epoch\n",
    "plot_hist(history_dict, ('accuracy', 'val_accuracy'), legends=('train', 'validation'), title='Accuracy', y_label='accuracy ->', x_label='epochs ->', save_to=f'Plots/short_chunk_cnn_{epochs}_acuracy')\n",
    "plot_hist(history_dict, ('loss', 'val_loss'), legends=('train', 'validation'), title='Loss', y_label='loss ->', x_label='epochs ->', save_to=f'Plots/short_chunk_cnn_{epochs}_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lambda model, data: np.argmax(model.predict(data), axis=-1)\n",
    "\n",
    "#Loading in the model\n",
    "model = save_utils.load_model(model_name)\n",
    "\n",
    "# Training prediction\n",
    "y_pred_train = predict(model, mel_train)\n",
    "y_true_train = np.argmax(lab_train, axis= -1)\n",
    "print(f\"ACCURACY FOR TRAIN SET {accuracy_score(y_true_train, y_pred_train)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='micro')*100:.4f} %\")\n",
    "\n",
    "\n",
    "# Validation prediction\n",
    "y_pred_val = predict(model, mel_val)\n",
    "y_true_val = np.argmax(lab_val, axis= -1)\n",
    "print(f\"ACCURACY FOR VAL SET {accuracy_score(y_true_val, y_pred_val)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='micro')*100:.4f} %\")\n",
    "\n",
    "# Test prediction\n",
    "y_pred_test = predict(model, mel_test)\n",
    "y_true_test = np.argmax(lab_test, axis= -1)\n",
    "print(f\"ACCURACY FOR TEST SET {accuracy_score(y_true_test, y_pred_test)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='micro')*100:.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = save_utils.get_class_names()\n",
    "\n",
    "#Confusion matrix of the predicted labels versus the true labels\n",
    "conf_mat = confusion_matrix(y_true_test, y_pred_test, normalize= 'true')\n",
    "conf_mat = np.round(conf_mat, 2)\n",
    "\n",
    "conf_mat_df = pd.DataFrame(conf_mat, columns=class_names, index=class_names)\n",
    "\n",
    "plot_conf_mat(conf_mat_df, save_to=f\"Plots/{model_name}{epochs}_test_conf_mat.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-industry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8db3ad7877ac29be891f762e2f8f2bd0b14f50820e5ea7ad73e7636a7ffe5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
