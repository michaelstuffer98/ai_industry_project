{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "This file traines a recurrent neural network on the melspectogram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 16:00:27.506159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from lib_util import utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define crossentropy and accuracy metric for the training routine\n",
    "\n",
    "For the metric we use a binary accuracy, for the loss a binary cross-entropy loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "define the RNN model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformPredictor:\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        i = np.random.randint(0, self.n_classes)\n",
    "        return np.random.random((X.shape[0], self.n_classes))\n",
    "\n",
    "class MajorityPredictor:\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.most_common_class = -1\n",
    "    \n",
    "    def train(self, y):\n",
    "        self.most_common_class = np.argmax(np.sum(y, axis=0))\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0], self.n_classes))\n",
    "        preds[:, self.most_common_class] = 1\n",
    "        return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "Load the configuration for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config('rnn')\n",
    "\n",
    "# Extract the values\n",
    "model_name = config['model_name']\n",
    "batch_size = config['batch_size']\n",
    "epochs = config['epochs']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "\n",
    "Load the numpy arrays and the label-class mapping. Split the data accordingly into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load slice from file melspec_features_001.npy\n",
      "Load slice from file melspec_features_002.npy\n",
      "Load slice from file melspec_features_003.npy\n",
      "Load slice from file melspec_features_004.npy\n",
      "Load slice from file melspec_features_005.npy\n",
      "Load slice from file melspec_features_006.npy\n",
      "Load slice from file melspec_features_007.npy\n",
      "Load slice from file melspec_features_008.npy\n",
      "Load slice from file melspec_features_009.npy\n",
      "Load slice from file melspec_features_010.npy\n",
      "Load slice from file melspec_features_011.npy\n",
      "Load slice from file melspec_features_012.npy\n",
      "Loaded 12 files as slices, resulting shape: (4068, 33152)\n"
     ]
    }
   ],
   "source": [
    "melspec_data = utils.load_sliced_numpy_array('melspec_features')\n",
    "labels = np.load('data/labels.npy')\n",
    "\n",
    "labels_to_id = utils.get_class_mapping()\n",
    "\n",
    "mel_train, mel_test_val, lab_train, lab_test_val = train_test_split(melspec_data, labels, train_size=config['train_set_size'], random_state=config['random_state'])\n",
    "mel_val, mel_test, lab_val, lab_test             = train_test_split(mel_test_val, lab_test_val, test_size=(config['val_set_size']/(1-config['train_set_size'])), shuffle=False)\n",
    "\n",
    "# Check the shapes of the splitted sets\n",
    "assert mel_train.shape[0] == lab_train.shape[0] and mel_test.shape[0] == lab_test.shape[0] and mel_val.shape[0] == lab_val.shape[0]\n",
    "assert mel_train.shape[1] == mel_test.shape[1] == mel_val.shape[1] and lab_train.shape[1] == lab_test.shape[1] == lab_val.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the naive predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FOR TRAIN SET 19.3888 %\n",
      "MACRO F1 SCORE FOR TRAIN SET 19.3537 %\n",
      "MICRO F1 SCORE FOR TRAIN SET 19.3888 %\n",
      "WEIGHTED F1 SCORE FOR TRAIN SET 19.4442 %\n",
      "\n",
      "ACCURACY FOR VAL SET 23.2787 %\n",
      "MACRO F1 SCORE FOR VAL SET 22.9621 %\n",
      "MICRO F1 SCORE FOR VAL SET 23.2787 %\n",
      "WEIGHTED F1 SCORE FOR TRAIN SET 23.6115 %\n",
      "\n",
      "ACCURACY FOR TEST SET 20.9493 %\n",
      "MACRO F1 SCORE FOR TEST SET 20.8653 %\n",
      "MICRO F1 SCORE FOR TEST SET 20.9493 %\n",
      "WEIGHTED F1 SCORE FOR TEST SET 21.1428 %\n"
     ]
    }
   ],
   "source": [
    "model = UniformPredictor(len(labels_to_id))\n",
    "\n",
    "# read in the config\n",
    "pp_config = utils.get_config('preprocess')\n",
    "n_mels = pp_config['melspectogram']['n_mels']\n",
    "\n",
    "mel_test = mel_test.reshape(mel_test.shape[0], -1, n_mels)\n",
    "\n",
    "predict = lambda model, data: np.argmax(model.predict(data), axis=-1)\n",
    "\n",
    "mel_test = mel_test.reshape(mel_test.shape[0], -1, n_mels)\n",
    "\n",
    "#Loading in the model\n",
    "# model = utils.load_model(model_name)\n",
    "\n",
    "# Training prediction\n",
    "y_pred_train = predict(model, mel_train)\n",
    "y_true_train = np.argmax(lab_train, axis= -1)\n",
    "print(f\"ACCURACY FOR TRAIN SET {accuracy_score(y_true_train, y_pred_train)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='micro')*100:.4f} %\")\n",
    "print(f\"WEIGHTED F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='weighted')*100:.4f} %\")\n",
    "print()\n",
    "\n",
    "# Validation prediction\n",
    "y_pred_val = predict(model, mel_val)\n",
    "y_true_val = np.argmax(lab_val, axis= -1)\n",
    "print(f\"ACCURACY FOR VAL SET {accuracy_score(y_true_val, y_pred_val)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='micro')*100:.4f} %\")\n",
    "print(f\"WEIGHTED F1 SCORE FOR TRAIN SET {f1_score(y_true_val, y_pred_val, average='weighted')*100:.4f} %\")\n",
    "print()\n",
    "\n",
    "# Test prediction\n",
    "y_pred_test = predict(model, mel_test)\n",
    "y_true_test = np.argmax(lab_test, axis= -1)\n",
    "print(f\"ACCURACY FOR TEST SET {accuracy_score(y_true_test, y_pred_test)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='micro')*100:.4f} %\")\n",
    "print(f\"WEIGHTED F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='weighted')*100:.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FOR TRAIN SET 24.2712 %\n",
      "MACRO F1 SCORE FOR TRAIN SET 7.8123 %\n",
      "MICRO F1 SCORE FOR TRAIN SET 24.2712 %\n",
      "WEIGHTED F1 SCORE FOR TRAIN SET 9.4807 %\n",
      "\n",
      "ACCURACY FOR VAL SET 21.6393 %\n",
      "MACRO F1 SCORE FOR VAL SET 7.1159 %\n",
      "MICRO F1 SCORE FOR VAL SET 21.6393 %\n",
      "WEIGHTED F1 SCORE FOR TRAIN SET 7.6992 %\n",
      "\n",
      "ACCURACY FOR TEST SET 26.0229 %\n",
      "MACRO F1 SCORE FOR TEST SET 8.2597 %\n",
      "MICRO F1 SCORE FOR TEST SET 26.0229 %\n",
      "WEIGHTED F1 SCORE FOR TEST SET 10.7471 %\n"
     ]
    }
   ],
   "source": [
    "model = MajorityPredictor(len(labels_to_id))\n",
    "\n",
    "# read in the config\n",
    "pp_config = utils.get_config('preprocess')\n",
    "n_mels = pp_config['melspectogram']['n_mels']\n",
    "\n",
    "mel_test = mel_test.reshape(mel_test.shape[0], -1, n_mels)\n",
    "\n",
    "model.train(lab_train)\n",
    "\n",
    "predict = lambda model, data: np.argmax(model.predict(data), axis=-1)\n",
    "\n",
    "mel_test = mel_test.reshape(mel_test.shape[0], -1, n_mels)\n",
    "\n",
    "#Loading in the model\n",
    "# model = utils.load_model(model_name)\n",
    "\n",
    "# Training prediction\n",
    "y_pred_train = predict(model, mel_train)\n",
    "y_true_train = np.argmax(lab_train, axis= -1)\n",
    "print(f\"ACCURACY FOR TRAIN SET {accuracy_score(y_true_train, y_pred_train)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='micro')*100:.4f} %\")\n",
    "print(f\"WEIGHTED F1 SCORE FOR TRAIN SET {f1_score(y_true_train, y_pred_train, average='weighted')*100:.4f} %\")\n",
    "print()\n",
    "\n",
    "# Validation prediction\n",
    "y_pred_val = predict(model, mel_val)\n",
    "y_true_val = np.argmax(lab_val, axis= -1)\n",
    "print(f\"ACCURACY FOR VAL SET {accuracy_score(y_true_val, y_pred_val)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR VAL SET {f1_score(y_true_val, y_pred_val, average='micro')*100:.4f} %\")\n",
    "print(f\"WEIGHTED F1 SCORE FOR TRAIN SET {f1_score(y_true_val, y_pred_val, average='weighted')*100:.4f} %\")\n",
    "print()\n",
    "\n",
    "# Test prediction\n",
    "y_pred_test = predict(model, mel_test)\n",
    "y_true_test = np.argmax(lab_test, axis= -1)\n",
    "print(f\"ACCURACY FOR TEST SET {accuracy_score(y_true_test, y_pred_test)*100:.4f} %\")\n",
    "print(f\"MACRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='macro')*100:.4f} %\")\n",
    "print(f\"MICRO F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='micro')*100:.4f} %\")\n",
    "print(f\"WEIGHTED F1 SCORE FOR TEST SET {f1_score(y_true_test, y_pred_test, average='weighted')*100:.4f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-industry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca8db3ad7877ac29be891f762e2f8f2bd0b14f50820e5ea7ad73e7636a7ffe5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
